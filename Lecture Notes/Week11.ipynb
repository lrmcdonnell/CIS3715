{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###**Document Analysis**\n",
        "- ex. Spam detection, review classification\n",
        "\n",
        "Given $n$ samples: $\\{(x_1,y_2), (x_2,y_2),...,(x_n,y_n)\\}$\n",
        "\n",
        "Goal: Learn a mapping function from $x$ to $y$\n",
        "\n",
        "Each sample could be:\n",
        "- an email (spam detection)\n",
        "- a paragraph (review classification)\n",
        "- an article\n",
        "\n",
        "1. Convert categorical features into numerical values\n",
        "  - Label encoding\n",
        "  - One-hot encoding\n",
        "  - Ordinal encoding\n",
        "\n",
        "How to convert words into numerical values?\n",
        "- Each sentence/paragraph contains multiple words\n",
        "- Bag-of-words!\n",
        "\n"
      ],
      "metadata": {
        "id": "bhjQm_Q-DrRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words**\n",
        "\n",
        "Can represent a sentence/paragraph/article as a bag of words vector\n",
        "\n",
        "Contains the set of unique words in a section of text to construct a vocabulary\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Build the vocabulary/dictionary from the given dataset\n",
        "  - get all unique words\n",
        "  - each word in the vocabulary has an index\n",
        "\n",
        "2. Represent each sentence/paragraph/article with the vocabulary\n",
        "  - use a vectore whose dimenionality equals the size of the vocabulary\n",
        "  - if the word appears, add 1 to the corresponding element in the vector\n",
        "\n",
        "Properties:\n",
        "- Can not preserve the order of the words\n",
        "- high dimensional\n",
        "- very sparse\n",
        "- some words are too common for all documents (it, the, ...)\n",
        "\n",
        "**Term Frequency-Inverse Document Frequency (TF-IDF)**\n",
        "- reflects how important a word is to a document in a collection\n",
        "- definition:\n",
        "  $TF(t,d) = \\frac{\\#t\\: in\\: document\\: d}{\\#words\\: in\\: document\\: d}$\n",
        "  \n",
        "  $IDF(t) = log\\frac{\\#documents}{\\#documents\\: containing\\: t}$\n",
        "  \n",
        "  $TF-IDF = TF(t,d) * IDF(t)$\n",
        "\n",
        "- Term Frequency (TF):\n",
        "  - measures the frequency of a word in a document\n",
        "\n",
        "- Inverse Document Frequency (IDF):\n",
        "  - measures the rareness of a word in all documents\n",
        "  - the more documents a word appears in, the less valuable that word is, as a signal to differentiate any given document"
      ],
      "metadata": {
        "id": "go7H_kYNF_as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two Classes**: binary classification - logistic regression, KNN, etc\n",
        "- imbalanced data: use recall, precision, F1\n",
        "\n",
        "**Multiple Classes**: multi-class logistic regression, KNN, etc\n",
        "- softmax\n",
        "- imbalanced data: use micro/macro recall, precision, F1\n",
        "\n",
        "###**Non-Negative Matrix Factorization (NMF)**\n",
        "\n",
        "Defn: $min||X-FG^T||^2_F$\n",
        "\n",
        "- columns of F are the underlying basis vectors\n",
        "- rows of G give the weights of the basis vectors\n",
        "- application: topic models\n",
        "\n",
        "###**Multiplicative Update Method**\n",
        "\n",
        "Most commonly used method\n",
        "\n",
        "The update rule: fix F, solve for G; fix G, solve for F\n",
        "\n",
        "- arises from gradient descent method\n",
        "\n",
        "$F_{ik} + ɛ_{ik}[(XG)_{ik} - (FG^TG)_{ik}]$\n",
        "\n",
        "set $ɛ_{ik} = \\frac{F_{ik}}{(FG^TG)_{ik}}$\n",
        "\n",
        "Then $F_{ik} = F_{ik}\\frac{(XG)_{ik}}{(FG^TG)_{ik}}$\n",
        "\n",
        "update F: ^\n",
        "update G: $G_{jk}\\frac{(X^TF)_{jk}}{(GF^TF)_{jk}}$\n",
        "\n",
        "until converges"
      ],
      "metadata": {
        "id": "9OA8D2N3ptgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Page Rank**\n",
        "\n",
        "Components of a graph:\n",
        "- nodes/vertices\n",
        "- edges/links\n",
        "- graph/network\n",
        "\n",
        "Types of graphs:\n",
        "- **directed** - links are directed\n",
        "- **undirected** - links are undirected\n",
        "\n",
        "Adjacency matrix: describes links between nodes\n",
        "\n",
        "Node degrees:\n",
        "- **undirected** - the number of edges adjacent to a node\n",
        "- **directed** -\n",
        "  - *in-degree*: number of head ends adjacent to a node\n",
        "  - *out-degree*: number of tail ends adjacent to a node\n",
        "\n",
        "page $i$ with importance $r_i$ has $d_i$ out-links, each link gets $\\frac{r_i}{d_i}$ votes\n",
        "\n",
        "page j's importance, $r_j$ is the sum of votes on its in-links\n",
        "\n",
        "importance of each node :\n",
        "  $r_j = \\sum{\\frac{r_i}{d_i}}$\n",
        "\n"
      ],
      "metadata": {
        "id": "rENbfp5dsbio"
      }
    }
  ]
}